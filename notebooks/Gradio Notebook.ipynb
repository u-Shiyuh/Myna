{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "#init Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    deployment=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME,\n",
    "    model=OPENAI_ADA_EMBEDDING_MODEL_NAME,\n",
    "    openai_api_base=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "    openai_api_type=\"azure\",\n",
    "    chunk_size=1,\n",
    ")\n",
    "llm = AzureChatOpenAI(\n",
    "deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "model_name=OPENAI_MODEL_NAME,\n",
    "openai_api_base=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "openai_api_key=OPENAI_API_KEY,\n",
    "openai_api_type=\"azure\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain({\"question\": \"Give me the prices and details\"})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain_1 = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "chain_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.document_loaders import GoogleDriveLoader\n",
    "from langchain.document_loaders import GCSFileLoader\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"1YVEiuDJPEL6Hbgtn64DCOJJgFXf-CEfZ\",\n",
    "    token_path='./google_token.json',\n",
    "    credentials_path = \"credentials.json\",\n",
    "    # Optional: configure whether to recursively fetch files from subfolders. Defaults to False.\n",
    "    recursive=False,\n",
    ")\n",
    "index_creator = VectorstoreIndexCreator(embedding=embeddings)\n",
    "docsearch = index_creator.from_loaders([loader])\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.vectorstore.as_retriever(), input_key=\"question\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "#init Azure OpenAI\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "model_name=OPENAI_MODEL_NAME,\n",
    "openai_api_base=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "openai_api_key=OPENAI_API_KEY,\n",
    "openai_api_type=\"azure\",\n",
    ")\n",
    "\n",
    "\n",
    "def random_response(message, history):\n",
    "    reply = chain({\"question\": message})\n",
    "    return reply[\"result\"]\n",
    "\n",
    "demo = gr.ChatInterface(random_response)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
